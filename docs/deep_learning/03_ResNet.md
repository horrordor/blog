# Introduction

ResNet(深度残差网络)，由何凯明提出。ResNet主要解决了深度网络退化问题，算是CNN图像史上的一件里程碑事件。ResNet也在ILSVR和COCO 2015图像挑战赛上取得了令人瞩目的成绩。ResNet取得了五项第一，并刷新了CNN模型在ImageNet数据集上的成绩。


# ResNet可以解决什么问题？

深度卷积神经网络是用来提取图片的特征的。几乎所有的视觉任务，比如分类，检测，分割都是基于提取的特征来进行学习的。所以，能提取到更好的特征就能获得更好的效果。如何通过深度卷积神经网络更好的提取特征？一个很直接的想法就是加深网络的深度。但是实验结果表明，在线性的卷积神经网络中仅仅增加网络的深度，会出现网络退化现象。

!!! note "网络退化现象:不同于梯度爆炸/消失，而是随着网络的深度的增加，网络的训练趋于饱和。甚至出现层数越多，训练误差越大的现象。"

batchnorm（BN）层的引入基本解决了plain 网络梯度消失和梯度爆炸的问题。如果不是过拟合和梯度消失导致的，那么什么原因导致模型“退化”呢？我们假设有一个浅层网络，我们通过堆积新层的方式来建立更深的网络。那么深层网络的解空间应该是包含浅层网络的解空间。如果让那些新增的层不做任何的学习，仅仅简单的复制浅层网络的特征，即新层做 **恒等映射（identity mapping** 。那么，在这种情况下，深层网络应该和浅层网络的性能一样，也不应该出现“退化现象”。**更好的解** 明明存在，为什么找不到？找到的反而是最差的解？

显然，这是个优化问题，反应出结构相似的模型，其优化难度是不一样的，且难度的增长并不是线性的， **越深的模型越难以优化** 。

有两种解决思想。一种是调整求解方法，比如更好的初始化方式，更好的梯度下降算法等；另一种则是调整模型结构，让模型更易于优化。

ResNet的作者从后者入手，探求更好的模型结构。将堆叠的几层layer称之为一个block，对于某个block，其可以拟合的函数为F(x)，如果期望的潜在映射为H(x)，与其让F(x) 直接学习潜在的映射，不如去学习残差H(x)−x，即F(x):=H(x)−x，这样原本的前向路径上就变成了F(x)+x，用F(x)+x来拟合H(x)。作者认为这样可能更易于优化，因为相比于让F(x)学习潜在映射，让F(x)学习成0要更加容易。这样，对于冗余的block，只需F(x)→0就可以得到恒等映射，性能不减。


看到这里，不由产生两个问题。

1.为什么残差学习更容易？

2.F(x)+x应该如何设计？

# 残差学习

在普通的卷积神经网络中，我们假设输入X，想要经过网络拟合H(x),即要学习一些参数，使得X经过网络后得到H(x)，而ResNet，学习的是残差，拟合的是残差F(x),F(x) = H(x) - X。从数学上讲，我们假设残差单元表示为：


$$ y_l = h(x_l) + F(x_l, W_l) $$

$$ x_{l+1} = f(y_l) $$

其中，

x~l~：第l层的输入

x~l+1~：第l层的输出

F( )：残差块

f( )：激活函数ReLU

h(x~l~)=x~l~：表示恒等映射，shotcut

!!! note "F( )残差块一般包含多层layer"

基于上述公式，可得从浅层l到深层L的特征为：

$$ x_L = x_l + \sum_{i=l}^{L - 1} F(x_i, W_i) $$

根据链式法则，可以求得方向过程的梯度：

$$ \frac{ \partial loss }{ \partial x_l } = \frac{ \partial loss }{ \partial x_L } *  \frac{ \partial x_L }{ \partial x_l } = \frac{ \partial loss }{ \partial x_L } * (1 + \frac{ \partial }{ \partial x_l } \sum_{i = l}^{L - 1}F(x_i, W_i ))$$

$\frac{ \partial loss }{ \partial x_l }$ 表示loss关于x_L的梯度；括号中，1表示两层之间进行无损传播。在反向梯度传播中，残差块中任意两层该项的梯度都等于1，可以有效的避免梯度消失和梯度爆炸。另一项残差梯度则需要经过带有权重（W）的层，其梯度值即使很小，但因为有1的存在，也不会导致梯度消失。

# 残差模块

在ResNet之前，主流的神经网络都是卷积层的线性堆叠。输入数据经过每个卷积层都会发生变化，产生一个输出的feature map。我们要训练的就是网络中的参数，使得输入，输出之间能够形成一个映射，即mapping。作者的思路是，引入一个identity mapping（恒等映射）跨越一些（至少一个）卷积层，然后网络只进行一个residual mapping（残差映射）。

其中 x和网络输出的F(x)必须维度一致（维度不同，直接采用1*1的卷积进行上/下采样）。在网络的输出阶段，先相加，在进行relu。

# 网络结构的特点

- 与plain net相比，ResNet多了很多“旁路”，即shortcut路径，其首尾圈出的layers构成一个Residual Block；
- ResNet中，所有的Residual Block都没有pooling层，降采样是通过conv的stride实现的；
- 分别在conv3_1、conv4_1和conv5_1 Residual Block，降采样1倍，同时feature map数量增加1倍，如图中虚线划定的block；
- 通过Average Pooling得到最终的特征，而不是通过全连接层；
- 每个卷积层之后都紧接着BatchNorm layer，为了简化，图中并没有标出；
- ResNet结构非常容易修改和扩展，通过调整block内的channel数量以及堆叠的block数量，就可以很容易地调整网络的宽度和深度，来得到不同表达能力的网络，而不用过多地担心网络的“退化”问题，只要训练数据足够，逐步加深网络，就可以获得更好的性能表现。


# 思考

理论上越深的层，如果看作在较浅层的神经网络的扩充，那么理论上可以做到完全的复制，但实际上并不能做到。之前的神经网络如果是想训练来拟合一个函数H(X)，那么残差神经网络是想训练来拟合函数F(X)=H(X)-X，这样训练出来的神经网络可以更容易做到恒等变换。H(X)就可以通过shortcut connection来计算。

## 残差模块的优点

- 不引入新的参数，不增加计算复杂度。
- 拟合的不再是underlying mapping ，而是 residual mapping。拟合起来要更简单更快。
- 采用残差模块，能通过增加网络深度，来提高网络的学习能力。
- 能够直接用SGD进行优化。

## 为什么能解决网络退化问题

- 恒等映射，这一路的梯度是1，这样就相当于把深层的梯度直接注入了底层，防止了梯度消失。
- 学习的是残差，是相对于本身出入x的偏差，学习能力更强。很容易拟合恒等映射，只要将网络中所有参数都设为0即可。
